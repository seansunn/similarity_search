{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b039e326-8659-4a6d-aa32-c4dbb40ef265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin C:\\ProgramData\\Miniconda\\envs\\qlora\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from langchain import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          AutoTokenizer,\n",
    "                          GenerationConfig,\n",
    "                          pipeline)\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1480240-47bc-46f1-8d99-07c47c02a248",
   "metadata": {},
   "source": [
    "1. https://api.python.langchain.com/en/latest/llms/langchain.llms.huggingface_pipeline.HuggingFacePipeline.html#langchain.llms.huggingface_pipeline.HuggingFacePipeline\n",
    "2. https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n",
    "3. https://huggingface.co/docs/transformers/main_classes/pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55de123-316c-4374-bb0e-a312087683dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039eba90551b43e8980f418519985f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"llms/vicuna-13b-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bit_compute_dtype=torch.float16\n",
    "    # ),\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    generation_config=GenerationConfig(\n",
    "                max_new_tokens=10,\n",
    "                top_p=0.0,\n",
    "                temperature=0.0\n",
    "    )\n",
    "    # max_new_tokens=10,\n",
    "    # temperature=0.0,\n",
    "    # top_p=0.0\n",
    ")\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d328ba3-297a-4eb5-92a8-8b42a1ac2fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_instruct = 'Answer the question directly based on the context below. It is important for me that you answer the question with one single word: \"Yes\", \"No\", or \"Maybe\". If the question cannot be answered using the information provided answer with \"No\".'\n",
    "\n",
    "context = '''- Objective: To assess whether eligibility to an adjuvant chemotherapy protocol in itself represents a good prognostic factor after radical cystectomy for bladder cancer.\n",
    "- Methods: Between April 1984 and May 1989, our institution entered 35 patients with invasive bladder cancer into the Swiss Group for Clinical and Epidemiological Cancer Research (SAKK) study 09/84. They were randomly assigned to either observation or three postoperative courses of cisplatin monotherapy after cystectomy. This study had a negative result. The outcome of these 35 patients (protocol group) was compared with an age- and tumor-stage-matched cohort (matched group; n = 35) who also underwent cystectomy during the same period, but were not entered into the SAKK study, as well as the remaining 57 patients treated during the study period for the same indication (remaining group).\n",
    "- Results: Median overall survival decreased from 76.3 months in the protocol group to 52.1 months in the matched group and to 20.3 months in the remaining group. The respective times of median recurrence-free survival were 67.2, 16.0, and 9.4 months. Tumor progression occurred in 46% of the protocol group compared with 69% in the matched group and 65% in the remaining group (P<.05). Cancer-related death was noted in 40% of the protocol group, 57% in the matched group, and 56% in the remaining group.'''\n",
    "\n",
    "query = \"Question: Is eligibility for a chemotherapy protocol a good prognostic factor for invasive bladder cancer after radical cystectomy?\"\n",
    "\n",
    "context_template = '''- Objective: [{objective}]\\n- Methods: [{methods}]\\n- Results: [{results}]'''\n",
    "query_template = '''Question: {query}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944ab8c9-6027-4f5f-832b-d4cecca6a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_template = \"\"\"{start}{instruction_begin}{system_begin}{system_msg}{system_end}\n",
    "{user}{user_instruct}\\n\n",
    "{query}\\n\n",
    "Context: \\n{context}{instruction_end}{assistant}{answer}{end}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "535b96ea-d2bd-4c02-893e-9735de83b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(template, context, user_instruct, query, answer=\"\", system_msg=\"\", model=\"vicuna\", prompt_type=\"eval\"):\n",
    "\n",
    "    system_msg = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\" if system_msg ==\"\" else system_msg\n",
    "    vicuna_kwargs = {\n",
    "        \"start\": \"\",\n",
    "        \"instruction_begin\": \"\",\n",
    "        \"system_begin\": \"\",\n",
    "        \"system_end\": \"\\n\",\n",
    "        \"user\": \"USER: \",\n",
    "        \"instruction_end\": \"\\n\",\n",
    "        \"assistant\": \"\\nASSISTANT: \",\n",
    "        }\n",
    "\n",
    "    llama_kwargs = {\n",
    "        \"start\": \"<s>\",\n",
    "        \"instruction_begin\": \"[INST] \",\n",
    "        \"system_begin\": \"<</SYS>>\\n\",\n",
    "        \"system_end\": \"\\n<</SYS>>\\n\",\n",
    "        \"user\": \"\",\n",
    "        \"instruction_end\": \" [/INST] \",\n",
    "        \"assistant\": \"\",\n",
    "        }\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\n",
    "            \"start\", \"end\", \"instruction_begin\", \"instruction_end\",\n",
    "            \"system_begin\", \"system_end\", \"system_msg\", \"user\",\n",
    "            \"user_instruct\", \"context\", \"query\", \"assistant\", \"answer\", \"end\"\n",
    "        ],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    current_kwargs = vicuna_kwargs if model == \"vicuna\" else llama_kwargs\n",
    "    prompt = prompt_template.format(\n",
    "        context=context,\n",
    "        query=query,\n",
    "        system_msg=system_msg,\n",
    "        user_instruct=user_instruct,\n",
    "        answer=\"\" if prompt_type == \"eval\" else answer,\n",
    "        end=\"\" if prompt_type == \"eval\" else \" </s>\",\n",
    "        **current_kwargs\n",
    "    )\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a1c1bac-0517-43d7-86ef-be01f220c1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "\n",
      "USER: Answer the question directly based on the context below. It is important for me that you answer the question with one single word: \"Yes\", \"No\", or \"Maybe\". If the question cannot be answered using the information provided answer with \"No\".\n",
      "\n",
      "Question: Is eligibility for a chemotherapy protocol a good prognostic factor for invasive bladder cancer after radical cystectomy?\n",
      "\n",
      "Context: \n",
      "- Objective: To assess whether eligibility to an adjuvant chemotherapy protocol in itself represents a good prognostic factor after radical cystectomy for bladder cancer.\n",
      "- Methods: Between April 1984 and May 1989, our institution entered 35 patients with invasive bladder cancer into the Swiss Group for Clinical and Epidemiological Cancer Research (SAKK) study 09/84. They were randomly assigned to either observation or three postoperative courses of cisplatin monotherapy after cystectomy. This study had a negative result. The outcome of these 35 patients (protocol group) was compared with an age- and tumor-stage-matched cohort (matched group; n = 35) who also underwent cystectomy during the same period, but were not entered into the SAKK study, as well as the remaining 57 patients treated during the study period for the same indication (remaining group).\n",
      "- Results: Median overall survival decreased from 76.3 months in the protocol group to 52.1 months in the matched group and to 20.3 months in the remaining group. The respective times of median recurrence-free survival were 67.2, 16.0, and 9.4 months. Tumor progression occurred in 46% of the protocol group compared with 69% in the matched group and 65% in the remaining group (P<.05). Cancer-related death was noted in 40% of the protocol group, 57% in the matched group, and 56% in the remaining group.\n",
      "\n",
      "ASSISTANT:  </s>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_formatter(qa_template, context, user_instruct, query, model=\"vicuna\", prompt_type=\"train\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7986733-dd5f-40d3-a872-17e7bc0e1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_answer_instruct = \"\"\"Answer the question directly based on the context below. It is important for me that you answer the question with one single word: \"Yes\", \"No\", or \"Maybe\". If the question cannot be answered using the information provided answer with \"No\".\"\"\"\n",
    "complex_answer_instruct = \"\"\"Answer the question directly based on the context below. It is important for me that you answer the question first with \"Yes\", \"No\", or \"Maybe\", and then explain your answer. If the question cannot be answered using the information provided answer with \"I do not know\".\"\"\"\n",
    "keyword_generation_instruct = \"\"\"Generate keywords that are relevant to the context below.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c154433e-bbf2-4f76-ad87-f7ebb04d6902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question directly based on the context below. It is important for me that you answer the question first with \"Yes\", \"No\", or \"Maybe\", and then explain your answer. If the question cannot be answered using the information provided answer with \"I do not know\".\n"
     ]
    }
   ],
   "source": [
    "print(complex_answer_instruct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a26990f4-01ba-479b-8161-bc422e7442e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes.\n"
     ]
    }
   ],
   "source": [
    "print(hf(prompt_formatter(qa_template, context, user_instruct, query, model=\"vicuna\", prompt_type=\"eval\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849200a-3760-4c92-b4c7-3df56cb9bdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
